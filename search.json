[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "notes for myself on things I‚Äôm learning about"
  },
  {
    "objectID": "posts/profiling-quantum-compilers/index.html",
    "href": "posts/profiling-quantum-compilers/index.html",
    "title": "Profiling Quantum Compilers: Introduction",
    "section": "",
    "text": "As part of my work on ucc, I‚Äôve spent most of my time to date on benchmarking. In the companion repository ucc-bench, we‚Äôve collected a small suite of representative circuits and built tooling to automate benchmarking performance of quantum compilers, especially as new versions are released.\nRecently, my focus has shifted more toward the internals of quantum compilation, and I wanted to get a deeper understanding of how these compilers actually work. I‚Äôve read papers and stepped through code with a debugger, but the performance angle has been nagging at me. We‚Äôre seeing a trend of moving core compiler optimization passes to high-performance languages like Rust 1, so I figured a great way to learn why is to see where today‚Äôs compilers spend their time. By profiling a compiler on a benchmark circuit, we can gain insight into both architecture and performance bottlenecks.\nThis is the first post in a series where I‚Äôll explore the performance of quantum compilers by putting them under the profiler."
  },
  {
    "objectID": "posts/profiling-quantum-compilers/index.html#profiling-quantum-compilers",
    "href": "posts/profiling-quantum-compilers/index.html#profiling-quantum-compilers",
    "title": "Profiling Quantum Compilers: Introduction",
    "section": "Profiling Quantum Compilers",
    "text": "Profiling Quantum Compilers\nIn this post, I briefly introduce profiling Python with the amazing py-spy tool. I won‚Äôt go into exhaustive detail on profiling, but I want to provide just enough background (by way of an example) to understand the results in later posts. I‚Äôll also give a brief overview of the repository that contains the code and scripts used to generate the compiler profiling data.\n\nProfiling and py-spy\nProfiling is the analysis of a program‚Äôs execution to understand where it‚Äôs spending its resources. For our purposes, we care about one primary resource: time. We want to know which functions are making the compiler slow.\npy-spy is a fantastic sampling profiler for Python with minimal overhead. It captures a sample of a running program‚Äôs call stack at a regular frequency (say 100 times per second). By aggregating thousands of these samples, py-spy builds a statistical picture of where the program spends its time. A crucial feature for our work is the --native flag2. This lets py-spy record non-Python parts of the call stack, letting us look inside the compilers that mix Python with native code.\n2¬†Unfortunately, --native is only supported on linux/windows and not my Mac :(. That‚Äôs why I used Github codespaces below.\nAn example\nLet‚Äôs look at an example adapted from a classic post on how profilers can be misleading. Turns out modern sampling profilers like py-spy handle the issue in that post, but its a good way to explore the visualizations py-spy generates.\nConsider this Python script\ndef work(n):\n    # This is a stand-in for a real computational task.\n    i = 0\n    while i &lt; n:\n        i += 1\n\ndef easy_task():\n    # A task that calls 'work' with a small input.\n    work(100_000)\n\ndef hard_task():\n    # A task that calls 'work' with a much larger input.\n    work(1_000_000)\n\ndef main():\n    while True:\n        easy_task()\n        hard_task()\n\nif __name__ == \"__main__\":\n    main()\nIn this code, both easy_task() and hard_task() call the same function, work(), but with vastly different workloads. A simpler profiler that only tracks how much time is spent on specific lines of code would just tell us work() is slow, but not why. A sampling profiler like py-spy, by virtue of collecting the entire call stacks, tells us both where the program is but also how it got there. This means it can distinguish call paths that get to work(), differentiating calls that come by way of easy_task() vs hard_task().\nLet‚Äôs see this in action. Assuming the above is saved to a main.py, you would run3 the command below for a few seconds and then kill the process:\n3¬†On my mac, I need to run this as sudo for py-spy to be able to introspect the running process. I didn‚Äôt use --native since this is pure python.$  py-spy record --format speedscope -o profile.json --function -- python main.py\nHere, we tell py-spy to record a profile of python main.py and save it to the file profile.json4. It uses the speedscope format, which you can upload to http://speedscope.app/ to visualize.\n4¬†I also passed --function to aggregate samples by the first line number of a function versus the sampled number. This way whether the sample is taken when we are at the i+=1 or the while i &lt;n: line in work(), the profile result groups them together. If we didn‚Äôt do this, the visualization would end up creating an entry for each distinct line-number of work() sampled and make it harder to analyze the charts.Let‚Äôs walk through the speedscope view of these results:\n\nTime Order View\n This is the default view when you first load the profile in speedscope. Time flows from left to right, and along the bottom, you see the the call stack at each point in time. So the blue bar with main() covers the entire view, then beneath it you see the alternating calls of hard_task() and easy_task() (although easy is too narrow to see its name!). The width of each box corresponds to the CPU time of that function call5. Mousing over a function creates a pop-up with the specific duration and source file for that call. Clicking on a function opens a call-stack view at the bottom, and shows time spent on this specific instance of the function call and aggregate time spent on all such function calls. There‚Äôs not a ton more to glean from this simple example, since the code is small enough that we already know a lot about its execution.\n5¬†That is, the time spent in that function call and it‚Äôs descendant calls. This might be much longer than the time spent actually in that function itself (the self-time) if it mostly calls into other functions.For more complex applications, the time ordered view is a great way to understand the flow of the program, where wider rectangles guide you to the longer running parts of the program. I find it useful to start with this view to confirm your profile captured what you intended, and then to orient yourself around ‚Äúgoldilocks‚Äù function calls. These are functions that aren‚Äôt super deep in the call stack that its unclear how you got there, but are also not too high-level that they are basically the whole program. Somewhere in the middle is just right.\n\n\nLeft Heavy View\n\nThe left heavy view throws out the time-ordered aspects, groups similar call stacks together, and then puts the heavier (longest running) stacks to the left of the plot. It has the same mouse-over and click details as the time-order view. Use this view to see where the program is spending its time overall, regardless of the order of execution. For this example, it nicely shows most time in main() is spent on hard_task() vs easy_task(), even though there are corresponding calls to work() below each. This view is useful when there are lots of interleaved function calls but you just care about the aggregate time spent in each alternate.\n\n\nSandwich View\nThis last view has two useful aspects. The first is the table in the image above. This shows the total time and self-time spent in a function call, and as a percent of the entire profile result.\n\nTwo Times\n Functions with large/large percent of self-time are hotspots to focus on for optimization. Self-time means the CPU is busy working directly in that function, so you can consider zooming in and doing some micro-benchmarking and optimizations on just that function. For our simple example, it nicely shows that the CPU is really spending its time in that silly loop in work(). If that were real code though, finding a way to optimize it would nicely improve overall performance.\nFunctions with large/large percent of total-time indicate the primary paths of program execution. This is the total CPU time spent where this function was somewhere in the call stack, but not necessarily where the program was running when sampled. So by definition, main() and other high-level functions have large percentages, but that doesn‚Äôt really help you understand much. Instead, you would want to scroll down this list to get to the aforementioned goldilocks level. For me, this is when I get to a function and go ‚Äúhuh, I‚Äôm surprised it spent that much time here‚Äù. For our simple example, we see that hard_task is indeed ten times heavier than easy_task.\n\n\nü•™ü•™ü•™ü•™ü•™ü•™ü•™ (the actual sandwich)\n If you click on a row in this list, you get the sandwich view shown above. This view splits the timeline around that function, effectively filtering the full time-ordered view to just callers and callees of that function. Not suprisingly, this is useful to zoom in on a function and get a sense of where it falls in the program execution. I find this especially useful to understand the architecture of the program.\n\n\n\n\n\nSetup\nWith that whirlwind tour of profiling behind us, how will we explore quantum compilers? https://github.com/bachase/quantum-compiler-profiling is a repository with all you need!\nThe code uses a 100-qubit QFT circuit in ucc-bench (by way of benchpress and QASMBench). The script in main.py then transpiles/compiles this circuit via Qiskit, ucc, pytket, Cirq, or pyqpanda3. There‚Äôs a corresponding run_bench.sh to generate the profiles using py-spy, and there are sub-directories with some profiles checked in6. I generated these profiles under a GitHub codespace (which you can do yourself by forking within GitHub). A tad more detail is in the README, but the upshot is we will use speedscope to explore each compiler‚Äôs results in the native.profiles directory in future posts.\n6¬†I ran profiles with & without --function to aggregate samples to start of functions. I don‚Äôt a priori know much about each function. Some might be lengthy with lots of internal loops, so I‚Äôd rather keep samples of different lines separate to potentially identify that structure. But having the aggregated view with --function will also help coarse grain over those details when first analyzing results.The QFT circuit was chosen based in recent ucc-bench performance results:\n\n\n\n\n\n\n\n\n\n\n\nCompiler\nbenchmark_id\nraw_multiq_gates\ncompile_time_ms\ncompiled_multiq_gates\n\n\n\n\nucc\nqft\n10050\n478.4261\n2740\n\n\npytket-peep\nqft\n10050\n48776.9968\n4498\n\n\nqiskit-default\nqft\n10050\n509.0576\n3570\n\n\ncirq\nqft\n10050\n32392.2503\n4648\n\n\npyqpanda3\nqft\n10050\n130.838\n2740\n\n\n\nThere is some reduction in two-qubit gates, and the circuit takes a non-trivial amount of compile time. So something interesting is happening!\nAI Disclaimer: I‚Äôve used LLMs to review and give feedback on this post, but the original content was written by me."
  },
  {
    "objectID": "posts/profiling-quantum-compilers-pytket/index.html",
    "href": "posts/profiling-quantum-compilers-pytket/index.html",
    "title": "Profiling Quantum Compilers: pytket",
    "section": "",
    "text": "For the next post in the quantum compiler profiling series, we will focus on pytket version 2.9.1. pytket is ‚Äúa python module for interfacing with tket, a quantum computing toolkit and optimising compiler developed by Quantinuum.‚Äù At the time of writing, Quantinuum just released their next generation quantum software stack, which includes tket2. But it‚Äôs still a work in progress, so for now, I will focus on the existing version."
  },
  {
    "objectID": "posts/profiling-quantum-compilers-pytket/index.html#overview",
    "href": "posts/profiling-quantum-compilers-pytket/index.html#overview",
    "title": "Profiling Quantum Compilers: pytket",
    "section": "Overview",
    "text": "Overview\nAs we did last time, we begin with a top-level view of the compilation. This took a total of 1 minute and 10 seconds. You can inspect this function-aggregated view in speedscope here.\n\nThe benchmark harness code is below, with the code annotated with the labels in the image above. \ndef run_pytket(qasm):                                               #(1)\n    from pytket.passes import (\n        SequencePass,\n        AutoRebase,\n        FullPeepholeOptimise,\n    )\n    from pytket.predicates import CompilationUnit\n    from pytket.circuit import OpType\n    from pytket.qasm import circuit_from_qasm_str\n\n    compilation_unit = CompilationUnit(circuit_from_qasm_str(qasm)) #(2)\n    passes = [\n        FullPeepholeOptimise(),\n        AutoRebase({OpType.Rx, OpType.Ry, OpType.Rz, OpType.CX, OpType.H}),\n    ]\n    SequencePass(passes).apply(compilation_unit)                    #(3)\n    return compilation_unit.circuit\nThe bulk of the time is in two spots:\n\n\n2 [2%, 1.99s] in circuit_from_qasm_str parsing the 25K line QASM file into pytket‚Äôs in-memory circuit representation.\n3 [96%, 1m 8s] in apply actually optimizing the circuit.\n\n\nNot visible is the python import time, which at 320ms was less than half a percent of the runtime, but on the same order that Qiskit spends on imports.\n\nParsing QASM\nAlthough a small fraction of the overall runtime, I am curious to explore how tket is parsing QASM and how that compares to other libraries. Zooming in on that segment in the timeline view:\n\nAt the highest-level, 1 is the circuit_from_qasm_str function in pytket, reproduced here:\ndef circuit_from_qasm_str(qasm_str: str, maxwidth: int = 32) -&gt; Circuit: # (1)\n    global g_parser  # noqa: PLW0602\n    ...\n    #                 (3)             (2)\n    #                  ^               ^\n    circ = Circuit.from_dict(g_parser.parse(qasm_str))\n    cpass = scratch_reg_resize_pass(maxwidth) # &lt;-- (4)\n    cpass.apply(circ)\n    return circ\nHere, g_parser is an instance of a lark parser. Lark is pure-python parser library. The grammar1 for QASM2 parsing in pytket is here. What is most interesting is what representation tket parses into. The g_parser.parse method at 2 takes in a QASM string and returns a python dict that is effectively JSON. Tket has a JSON format for circuits that is uses for serialization. For the QFT circuit we are compiling, this is (mostly) a series of commands, but not at all compact (repeating keys, numeric params as strings); an excerpt:\n1¬†Lark is also used in QuEra‚Äôs Bloqade circuit dialect for parsing qasm.{\"commands\":[{\"args\":[[\"q\",[99]]],\"op\":{\"type\":\"Ry\",\"params\":[\"((pi / 2))/pi\"]}},{\"args\":[[\"q\",[99]]],\"op\":{\"type\":\"Rx\",\"params\":[\"(pi)/pi\"]}},{\"args\":[[\"q\",[99]]],\"op\":{\"type\":\"Rz\",\"params\":[\"((pi / 4))/pi\"]}},{\"args\":[[\"q\",[99]],[\"q\",[98]]],\"op\":{\"type\":\"CX\"}},{\"args\":[[\"q\",[98]]],\"op\":{\"type\":\"Rz\",\"params\":[\"(((-pi) / 4))/pi\"]}},....\nAs a result, what started as 427 kilobyte QASM file expands to 8.6 megabyte JSON blob!\nIn 3, the Circuit.from_dict call converts the python dictionary to a JSON string (again overhead!) and ultimately parses it into the in-memory circuit representation. Even though these layers are in C++, there‚Äôs just a lot of overhead parsing all those strings (and I imagine was a partial motivation for Jeff).\nLastly, in 4, there is a pass to ensure all classical registers bit width is below a desired target. I‚Äôm guessing this is to better map to hardware, e.g.¬†64-bit classical registers make sense, 774-bit registers don‚Äôt.\nThe take-away I have here is that even transient intermediary representations that might make sense as small scale/for human use (JSON) end up with performance implications at relatively modest scales.\n\n\nCompiling\nOnwards to the fun stuff.\n\nTo understand the profile, let‚Äôs look back at benchmark harness code. This uses two passes from pytket\n\n\n1 FullPeepholeOptimize ‚Äúpass applies Clifford simplifications, commutes single-qubit gates to the front of the circuit and applies passes to squash subcircuits of up to three qubits. This provides a one-size-approximately-fits-all ‚Äúkitchen sink‚Äù solution to Circuit optimisation.‚Äù (from docs). Peephole optimization is a concept from classical compilers, where you look at small program segments (like looking through a peephole) and locally optimize them. 2\n2 AutoRebase converts to the target gateset we use in the benchmarks to ensure an apples-to-apples comparison. Note this only took 200ms, so isn‚Äôt actually visible on the image above. But I put the label where you‚Äôd end up finding it by zooming or Ctrl+F searching for ‚Äúrebase‚Äù.\n\n2¬†I suspect the debug symbols get a little messed up, as full_peephole_optimise does show up as 30ms of the call stack, but there are seconds of samples in the underlying passes that I see it calling from the source code. I don‚Äôt feel like building locally with full debug symbols, so assuming things map back as you‚Äôd expect.\n\nFull Peephole Optimize\nFortunately, there‚Äôs a nice single spot in the C++ source that shows what internal passes full peephole optimization uses. Reproducing here\nTransform full_peephole_optimise(bool allow_swaps,\n                                OpType target_2qb_gate) {\n  switch (target_2qb_gate) {\n    case OpType::CX:\n      return (\n          synthesise_tket() &gt;&gt; two_qubit_squash(false) &gt;&gt;\n          clifford_simp(allow_swaps) &gt;&gt; synthesise_tket() &gt;&gt;\n          two_qubit_squash(allow_swaps) &gt;&gt; three_qubit_squash() &gt;&gt;\n          clifford_simp(allow_swaps) &gt;&gt; synthesise_tket());\n    /* snip */\n  }\n}\nBy switching to the left-heavy 3 view, we see three_qubit_squash 1 is by far the longest running at 52 seconds. The mouse-overed two_qubit_swash 2 (which is called twice) runs for 4.5 seconds.\n3¬†Recall, the left-heavy view is not time-ordered, and insteads pushes the longer running segments of the sampled call stack to the left of the chart.\nFocusing on three_qubit_squash, it is defined here in C++, and is also documented as a standalone pass\n\nSquash three-qubit subcircuits into subcircuits having fewer CX gates, when possible, and apply Clifford simplification.\n\nLooking at the C++ code, it assumes all quantum operations in the circuit are single qubit or CNOT gates. It uses QISystem to track sets of 3-qubit interactions (represented by QInteraction). The pass walks the DAG representation of the circuit in topological order. For each quantum operation, it checks if it overlaps with an existing set of 3-qubit interactions. If so, it combines with them. If its a fresh interaction independent of any prior ones, it starts a new QInteraction. Otherwise, its an operation that would cause an existing QInteraction to exceed 3-qubits (so overlaps with say 1 of the existing 3 qubits). In this cases, it ‚Äúcloses‚Äù the interaction, taking the set of gates that had been combined to this interaction and replacing with an equivalent unitary decomposed to a canonical sequence of a few gates. So in short, contiguous runs of 3-qubit interactions get decomposed to a few gates. This is an interesting comparison to what we saw in the Qiskit post, where things like commutative cancellation/re-ordering via algebraic gate identities was the strategy. This pytket pass is more like a synthesis approach.\nThe close_interaction function doing that synthesis dominates the runtime. From there, time is spent in three_qubit_synthesis (20%) which does a numerical decomposition of three qubit gates and singleq_clifford_sweep() (40%) (care of clifford_simp) which leverages Clifford gate rules to simplify.\nMy only observation poking around this area is a few lengthy calls to PQPSquasher::flush. The PQPSquasher class ‚Äúsquashes chains of single qubit gates to minimal sequences of any two type of rotations (Rx, Ry or Rz), using Euler angle decompositions‚Äù (from here). Searching for this in the Sandwhich view of speedscope, I see 15 seconds of CPU time (22% of the entire runtime) here, even though only 110 ms of self-time in that function.\n\nFurther poking in the Callee side of the Sandwich view, I see a bunch of SymEngine:: functions, which is a third-party library for symbolic mathematics, that can be used as a backend for Sympy. For this context, it shows up when merging rotations in the PQPSquasher class (see total_angle in merge_rotations). I‚Äôm not going to dig much futher, but I‚Äôm not suprised there is some overhead for doing symbolic level mathematics. I don‚Äôt know if there is any caching or other performance optimization strategies with SymEngine. Perhaps there is a way to disable and switch to floating point calculations instead of that is sufficient for your use case. But it is interesting to the circuits Qiskit and tket generate:\n\ntketqiskit\n\n\nrz(3.998046875*pi) q[91];\ncx q[98],q[92];\nrz(0.015625*pi) q[93];\ncx q[97],q[94];\nrz(3.75*pi) q[95];\ncx q[99],q[91];\nrz(3.9921875*pi) q[92];\ncx q[97],q[93];\nrz(0.0625*pi) q[94];\ncx q[96],q[95];\n\n\ncx q[93],q[92];\nrz(-3*pi/4) q[92];\nry(pi/2) q[92];\nrz(1.5707933307386703) q[92];\ncx q[99],q[91];\nrz(-pi/512) q[91];\ncx q[99],q[91];\nrz(pi/512) q[91];\ncx q[98],q[91];\nrz(-pi/256) q[91];\n\n\n\n\nAbove are random exceprts of in the middle of the compiled QFT circuit (exported to QASM). Both have single qubit rotation angles that are multiple of pi, but qiskit lagely keeps them as simple fractions (but not all!). tket has them as some floating point multiple of pi. So qiskit must have an faster internal representation that is either symbolic or tracks the fraction representation relative to pi.\n\n\n\nA final Bottoms up view\nOne last note on tket, check out the bottom‚Äôs up view sorted by self-time\n\nA little over 7 seconds is spent allocating and freeing memory and this is largely in C++ code. This is just a reminder that just using a compiled language is not a guarantee of speed. How you organize your data and use memory has a big impact4.\n4¬†One drive-by thing I noticed was a use of std::list in the PQPSquash code here. std::list is a linked list, so each item is a new memory allocation. Although the code does rely on re-ordering/removing elements in the list, I bet a flat memory structure like std::vector wouldn‚Äôt be too hard and could perform much better."
  },
  {
    "objectID": "posts/profiling-quantum-compilers-pytket/index.html#tldr",
    "href": "posts/profiling-quantum-compilers-pytket/index.html#tldr",
    "title": "Profiling Quantum Compilers: pytket",
    "section": "TL;DR",
    "text": "TL;DR\nThe vast majority of the runtime was spent in the FullPeepholeOptimise pass, particularly within the three_qubit_squash routine. This pass identifies contiguous blocks of 3-qubit interactions and re-synthesizes them into a more optimal gate sequence. A significant portion of this synthesis time is spent in symbolic math routines for simplifying single-qubit rotations.\nAlso of note is a 20x size overhead in parsing QASM; the 427 KB input file is first converted into an 8.6 MB JSON representation before being loaded into the final in-memory circuit.\nI‚Äôm looking forward to seeing what tket2 does to improve on these areas!\nAI Disclaimer: I‚Äôve used LLMs to review and give feedback on this post, but the original content was written by me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\n\n\n\n\n\n2025-10-01\n\n\nProfiling Quantum Compilers: pyqpanda\n\n\n\n\n\n\n2025-09-29\n\n\nProfiling Quantum Compilers: pytket\n\n\n\n\n\n\n2025-09-10\n\n\nProfiling Quantum Compilers: Qiskit\n\n\n\n\n\n\n2025-08-21\n\n\nProfiling Quantum Compilers: Introduction\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/profiling-quantum-compilers-qiskit/index.html",
    "href": "posts/profiling-quantum-compilers-qiskit/index.html",
    "title": "Profiling Quantum Compilers: Qiskit",
    "section": "",
    "text": "This post is the first follow-up in the quantum compiler profiling series, focusing on Qiskit version 2.1.1. Let‚Äôs dive right in!"
  },
  {
    "objectID": "posts/profiling-quantum-compilers-qiskit/index.html#overview",
    "href": "posts/profiling-quantum-compilers-qiskit/index.html#overview",
    "title": "Profiling Quantum Compilers: Qiskit",
    "section": "Overview",
    "text": "Overview\nWe begin with a top-level view of the compilation (or transpilation in Qiskit lingo). This took a total of 1.04 seconds. You can inspect this function-aggregated view in speedscope here. \nI‚Äôve labeled the 3 primary calls within the profiling function\ndef run_qiskit(qasm):\n    from qiskit import QuantumCircuit, transpile # (1)\n    circuit = QuantumCircuit.from_qasm_str(qasm) # (2)\n    return transpile(                            # (3)\n        circuit, basis_gates=[\"rz\", \"rx\", \"ry\", \"h\", \"cx\"], optimization_level=3\n    )\n\n\n1 [39%, 410ms] The call tree starting from _find_and_load importing Qiskit.\n2 [29%, 300ms] in from_qasm_str parsing the 25K line QASM file into Qiskit‚Äôs in-memory circuit representation.\n3 [25%, 260ms] in transpile actually optimizing the circuit.\n\n\nLet‚Äôs dig in a bit more.\n\nimport\nAlmost 40% of this run is on imports! I suspect this will be similar for most libraries we profile, as there is a lot of dynamic importing and logic these libraries are doing to support different features, lazily load some more complex dependencies etc. I‚Äôm not going to explore which import(s) are taking all this time but a good bit comes from Qiskit itself importing numpy as seen in the sandwich view here: \nBut I do want to highlight this overhead is meaningful for users! For interactive sessions like notebooks you only pay for this once, but in scripts (CI tests etc), you pay for this each time the script is run. This might explain the appeal of languages like Rust, where most of this is resolved at compile time.\nFor diving deeper, checkout this old but still solid reference on what Python does when you type from package import foo.\n\n\nParsing QASM\nNext up is from_qasm_str, which takes 30% of the overall runtime. For this exploration, I switched to the non-function aggregated sampling (speedscope here) because functions like from_bytecode are pretty lengthy. We want to know specifically where inside that function the code is spending time, which is lost with function aggregation. Looking at the sandwich view around from_qasm_str we have some structure\n\nFor 1 loads itself is defined in qasm2/__init__.py, reproduced in part here\ndef loads(...):\n    custom_instructions = list(custom_instructions)\n    return _parse.from_bytecode(\n        _qasm2.bytecode_from_string(\n            string,\n_qasm2.bytecode_from_string calls directly into Rust to generate some bytecode representation1 of the parsed QASM that must be easier to manage, and probably is a simpler post-validated representation. But importantly, that call itself doesn‚Äôt show up in any samples, so it must be pretty fast.\n1¬†Bytecode is an instruction set oriented view of a program that simplifies processing by an interpreter. Python itself compiles your code (at execution time) to bytecode before it is run by the Python interpreter. QASM doesn‚Äôt have an official bytecode specification, but converting it to bytecode before interpreting is a good strategy to separate parsing + validation from the downstream use.For 2 and 3, the samples we have are all in _parse.from_bytecode, where the bytecode is used to construct the in-memory circuit. The two spots are nearby lines here\nfor op in bytecode:\n    ...\n    if opcode == OpCode.Gate:\n            gate_id, parameters, op_qubits = op.operands # (2)\n            qc._append(\n                CircuitInstruction(gates[gate_id](*parameters),\n                [qubits[q] for q in op_qubits]) # (3)\n            )\n    ...\nLook at the functions called from these lines, that first __init__ is the initializer for an Rz gate, _append is adding a newly created gate to QuantumCircuit, and params is both setting and adding a parameter to the quantum instruction representing a gate. Almost all the hex addresses 4 are calls inside the CPython/libpython shared object file, but I didn‚Äôt have debug symbols around to know what they are. But down in 5, we see calls into the Qiskit Rust code to create the Rust instances of these circuit instructions that are then wrapped in Python. I‚Äôm not familiar yet with Qiskit internals, but there will always be some overhead to go back and forth to Rust to create each gate instance.\nI don‚Äôt have any deep insights, but am interested to see the parsing performance of other libraries. It‚Äôs also interesting to see the mix of Python and Rust in Qiskit, where the initial bytecode parse is super fast, but there is still work to improve the in-memory construction of circuit representations afterwards.\n\n\nCompiling (aka Transpiling)\nOk, finally showtime! Again, we return to the sandwich view. Note that this is ‚Äúleft-heavy‚Äù within the view, meaning the calls show are not time-ordered, but shifted left by longest call.\n\nQiskit has a sophisticated PassManager class for specifying the transpilation passes to run. Our example code here used the default passes with the highest optimization setting. The _run_workflow and execute calls in the callstack are the machinery to dispatch the different passes. But the actual work is done in a specific pass run function. The level 3 passes are defined here, but only two stand out in the profiling results 2.\n2¬†Note that the results shown here were sampled at 100 times per second, meaning each sample is 10 milliseconds. This transpile section was ~250ms, so we have 25 samples. The limiting sampling factor was the GitHub workspace machine, which had trouble keeping up at a higher rate (but which I used to get access to native stack trace details). I did run locally with 1000 times per second sampling (but no native symbol support on my Mac), which showed similar time spent in these functions.\nMinimumPoint\n1 is part of the MinimumPoint transformation pass. This is used by other iterative passes to see if successive runs have improved some circuit metrics. It was hard to infer what these passes were from the sampled callstacks, so I instead put a breakpoint in the debugger to take a peek when the MinimumPoint class was constructed. I walked up the stack to see the passmanager this is part of and found:  So this pass group is running a bunch of optimization passes, and this MinimumPoint class is checking when it‚Äôs converged to a local minimum ‚Äúbest‚Äù for depth and size. But the heavy part here is all in one line\n# ...\nelif score &lt; state.score:\n    state.since = 1\n    state.score = score\n    state.dag = deepcopy(dag) #&lt;-- this one\nOnce a better score is reached, a full copy of the entire DAG is captured. You can scroll below in speedscope to see all the calls this results in.\nGiven the Python object model, a true independent copy of an object requires copying each internal object as well. This is expensive because Python variables are fundamentally references (pointers) to objects in memory. A simple (or ‚Äúshallow‚Äù) copy of the DAG would just create a new list of pointers to the exact same gate objects. Modifying a gate in the new DAG would also modify it in the old one.\ndeepcopy, in contrast, must recursively walk the entire object graph‚Äîvisiting every single gate, parameter, and qubit reference‚Äîand create a brand-new, independent Python object for each one. This process involves a massive number of new object allocations and reference lookups, all managed by the Python interpreter, making it far slower than a single, contiguous memory copy you might see in a language like C.\nFor this large circuit, it‚Äôs a heavy operation. If the IR were represented in Rust with a less pointer-heavy structure (like a more flat memory layout), it could be possible to optimize this. But I‚Äôm guessing this is complex as long as parts of the IR are still managed as Python objects at a granular level, versus entirely represented in Rust.\n\n\nCommutative Cancellation\n2 is the other heavy pass spot, with two call stacks corresponding to commutative_cancellation which is (unsurprisingly) a Rust-accelerated pass that uses commutation relations to identify cancellation opportunites between gates. I haven‚Äôt tried compiling the Rust components of Qiskit with debug symbols to get the locations within the Rust code directly. But for the longer 30ms call (3 samples), 20 ms (2 samples) are taken when releasing memory in free. In the other, 10 ms (1 sample) of the 20ms are spent hashing, which I assume is a lookup for common gate pairs.\nIf you look in the time-ordered view you see there are two commutative cancellation pass runs surrounding the minimum point call. Perhaps there could be savings by retaining the analysis or hash lookup across calls.\n\n\n\nExtra Extra - Qiskit Transpiler Profiling\nQiskit nicely provides a callback hook to capture the impact of various passes, including time spent on passes. This is a nice domain-specific companion to the general profiling results presented here. Running this extra code, we see 3\n3¬†Qiskit has both analysis passes and transformation passes. The former calculates some properties of the DAG, but doesn‚Äôt modify it. The latter does modify it. For simplifying this table, I‚Äôve excluded analysis passes as they didn‚Äôt have much runtime in this example.\n\n\nPass\nTime (s)\nSize4\nDepth\n2Q Gates\n\n\n\n\nUnitarySynthesis\n0\n25100\n796\n10050\n\n\nHighLevelSynthesis\n0\n25100\n796\n10050\n\n\nBasisTranslator\n0.0004\n25100\n796\n10050\n\n\nElidePermutations\n0\n25100\n796\n10050\n\n\nRemoveDiagonalGatesBeforeMeasure\n0\n25100\n796\n10050\n\n\nRemoveIdentityEquivalent\n0.0006\n15380\n796\n10050\n\n\nInverseCancellation\n0.001\n8900\n796\n3570\n\n\nContractIdleWiresInControlFlow\n0\n8900\n796\n3570\n\n\nCommutativeCancellation\n0.0118\n7289\n698\n3570\n\n\nConsolidateBlocks\n0.0035\n7289\n698\n3570\n\n\nSplit2QUnitaries\n0\n7289\n698\n3570\n\n\nUnitarySynthesis\n0\n7289\n698\n3570\n\n\nHighLevelSynthesis\n0\n7289\n698\n3570\n\n\nBasisTranslator\n0.0006\n7289\n698\n3570\n\n\nMinimumPoint\n0\n7289\n698\n3570\n\n\nConsolidateBlocks\n0.0035\n7289\n698\n3570\n\n\nUnitarySynthesis\n0\n7289\n698\n3570\n\n\nRemoveIdentityEquivalent\n0.0001\n7289\n698\n3570\n\n\nOptimize1qGatesDecomposition\n0.0012\n7189\n696\n3570\n\n\nCommutativeCancellation\n0.0056\n7189\n696\n3570\n\n\nContractIdleWiresInControlFlow\n0\n7189\n696\n3570\n\n\nMinimumPoint\n0.0483\n7189\n696\n3570\n\n\nConsolidateBlocks\n0.0035\n7189\n696\n3570\n\n\nUnitarySynthesis\n0\n7189\n696\n3570\n\n\nRemoveIdentityEquivalent\n0.0001\n7189\n696\n3570\n\n\nOptimize1qGatesDecomposition\n0.0012\n7189\n696\n3570\n\n\nCommutativeCancellation\n0.0057\n7189\n696\n3570\n\n\nContractIdleWiresInControlFlow\n0\n7189\n696\n3570\n\n\nMinimumPoint\n0\n7189\n696\n3570\n\n\n\n4¬†Size here is the DAG/circuit size, which is the number of instructions.For each column, rows in bold had meaningful time duration or change in circuit metric. The most meaningful passes are\n\nRemoveIdentityEquivalent\n‚ÄúRemoves gates whose effect is close to an identity operation up to a global phase and up to the specified tolerance.‚Äù Looking at the QASM, we can directly see many small angle rotations that are close to identity:\nrz(pi/2097152);\nrz(0) q[14];\nThis doesn‚Äôt change the depth or the number of two qubit gates, but is quite a lot of single qubit gates!\n\n\nInverseCancellation\n‚ÄúCancel specific Gates which are inverses of each other when they occur back-to-back.‚Äù The only reduction in two-qubit gates! Doing nothing is always easier than doing gates that end up doing nothing! After the prior pass, these opportunities are successive cnots on the same qubits:\ncx q[99],q[79];\ncx q[99],q[79];\n\n\nCommutativeCancellation\n‚ÄúCancel the redundant (self-adjoint) gates through commutation relations.‚Äù This is the first reduction in gate depth and comes from being able to push gates around via commutation relations and then cancel them once close together. There‚Äôs not a local QASM excerpt to show for this conveniently. But looking at the Rust implementation, it looks like this pass treats each qubit as a wire, and then for commuting sets, pushes gates around into a canonical ordering (when they have well defined commutation relations), after which it then combines or cancels gates, replacing with a consolidated gate."
  },
  {
    "objectID": "posts/profiling-quantum-compilers-qiskit/index.html#tldr",
    "href": "posts/profiling-quantum-compilers-qiskit/index.html#tldr",
    "title": "Profiling Quantum Compilers: Qiskit",
    "section": "TL;DR",
    "text": "TL;DR\nOnly a quarter of the runtime was spent in actual transpilation. The rest was in importing Qiskit itself, and parsing the QASM.\nYou can see the power of the rust Rewrites in e.g.¬†parsing qasm to bytecode. But you can also see the rewrite in progress, where some overhead going back and forth between Python and Rust, and the Python memory model influencing things like copying a DAG.\nFor transpilation, most time was spent copying the Dag once a fixed point was reached on the optimization pass. By looking at Qiskit specific profiling output via a callback, we saw the few passes that had the largest impact on improvements. Most were applying fairly straightforward simplifications ‚Äì but those have a big impact on the size, depth and ultimate number of two-qubit gates!\nThanks to Nate for feedback on this post.\nAI Disclaimer: I‚Äôve used LLMs to review and give feedback on this post, but the original content was written by me."
  },
  {
    "objectID": "posts/profiling-quantum-compilers-pyqpanda/index.html",
    "href": "posts/profiling-quantum-compilers-pyqpanda/index.html",
    "title": "Profiling Quantum Compilers: pyqpanda",
    "section": "",
    "text": "For the next post in the quantum compiler profiling series, we will focus on pyqpanda3 version 0.3.1. pyqpanda3 is ‚Äúhigh-performance quantum programming framework that enhances quantum computing efficiency through optimized circuit compilation, an advanced instruction stream format (OriginBIS), and hardware-aware execution strategies‚Äù developed by OriginQ. Although freely available as a python package, it is closed source with the python code shipping compiled shared object files for supported platforms. I‚Äôve included it because it is very fast and performant on benchmarks, probably thanks to its internal represenation of quantum circuits as a compact binary instruction stream. There are more details in this arXiv paper from the OriginQ team, but even then, only at a high-level."
  },
  {
    "objectID": "posts/profiling-quantum-compilers-pyqpanda/index.html#overview",
    "href": "posts/profiling-quantum-compilers-pyqpanda/index.html#overview",
    "title": "Profiling Quantum Compilers: pyqpanda",
    "section": "Overview",
    "text": "Overview\nAs we did the last times, we begin with a top-level view of the compilation. This took a total of 560 milliseconds. Remember, we are sampling 100 times per second, so this is only 56 samples. You can inspect this function-aggregated view in speedscope here.\n\nThe benchmark harness code is below, with the code annotated with the labels in the image above.\ndef run_pyqpanda(qasm): #                             (1)\n    from pyqpanda3.intermediate_compiler import (\n        convert_qasm_string_to_qprog,\n    )\n    from pyqpanda3.transpilation import Transpiler\n\n    circuit = convert_qasm_string_to_qprog(qasm) #    (2)\n    transpiler = Transpiler()\n    return transpiler.transpile(                 #    (3)\n        circuit,\n        init_mapping={},\n        optimization_level=2,\n        basic_gates=[\"RX\", \"RY\", \"RZ\", \"H\", \"CNOT\"],\n    )\nThe bulk of the time is in two spots:\n\n\n2 [71%, 400ms] in convert_qasm_string_to_qprog parsing the 25K line QASM file into pyqpanda3‚Äôs in-memory circuit representation.\n3 [27%, 150ms] in transpile actually optimizing the circuit.\n\n\nNot visible is the python import time, which was fairly negligible at 10ms. The overall runtime is almost 4 times faster than the next best compiler benchmarked for this QFT circuit.\n\nParsing\npyqpanda3 is quite fast, but it‚Äôs notable that it spends more time parsing than transpiling the circuit.\n\nIn the sandwich view above, we see what is likely a hand-built parser + tokenizer. The hierarchy of calls to 1 show the parser side, where a program is comprised of statements, which can probably be quantum or classical. Our QASM file is all quantum ones that are gate calls with operands. Below those parsers in 2 you can see a call to CompilerQASM::QASMScanner::next that pulls out the next token for the parser.\nUnder the hood this is using C++ standard library streams for this. For example, the CompilerParser::Scanner::consumeNumberLiteral uses a C++ stringstream to read into the number. There is some overhead from streams dealing with locales (also in 3 when calling CompilerParser::Token::toString). Specifically, parsing will have to call locale facets (ctype, numpunct, num_get) to classify characters and handle decimal points/thousands separators that may differ by region. Streams and locales alos have bookkeeping on whitespace and stream state. These can involve a few layers of indirection (via virtual function dispatch) that are a tad heavy for these short parsing calls. I‚Äôm nitpicking here, but given QASM has a specification, you might get more performance here with a leaner/purpose-built parser.\n\n\nTranspiling\nWithout the source, it‚Äôs even harder to tell what is happening with transpilation, but we at least get a hint from the time-ordered view below1.\n1¬†When writing this section, I decided to re-run the profiler with a higher sampling rate of 400 times per second because the standard 100 times/second had lots of single sample call stack entries that might not‚Äôve been representative. Click here to view this version in speedscope. Note there was some impact to runtime as a result, but the relative durations of each phase of transpilation is similar and I prefer slightly more detailed callstacks.\nThis timeline view zooms in on the transpilation call, which has three main phases\n\n\n1 [22.5ms] is DAGQCircuit::from_qprog, which converts from a general representation to a DAG circuit, presumably to be a DAG-style IR for the style of circuit optimizations in pyqpanda.\n2 [66ms] in Transpiler::transpile. I see calls to OptimizationPass::unitary_synthesis. Of note is a final call to DAGQCircuit::batch_insert, which would potentially help performance by modifying the dag all at once at the end, versus doing it piece by piece as its walked\n3 [280ms] in BasicTranslationPass::merage_and_translation (typo is in the original binary). Here, I see calls like translate_cz_to_cx_h, translate_oracle_to_cz_u4, translate_u3_to_rx_rz. I also see a stack of calls for TwoQubitBasisDecomposer that has a Weyl decomposition. All of these are repeated a few times, suggesting some looping pass.\n\n\nUnfortunately, its hard to backout the overall strategy here. But it looks like some standard translation/substitution rules, and then some consolidation followed by some two-qubit decomposition/synthesis before repeating. The documentation states for this highest level optimization configuration, ‚Äúadvanced optimizations are performed, including matrix optimization, two-qubit gate cancellation, and single-qubit gate merging.‚Äù\n\n\nDemangling and dependencies\nBefore wrapping up, I did want to poke a little bit more at the shared object files to learn a little more of what is used. For my mac, running the code below demangles the symbols of the shared library and then collects them by C++ namespace and name. Note this just looks at the transpilation module from pyqpanda3.\nnm --demangle transpilation.cpython-313-darwin.so \\\n  | awk '{print $3}' \\\n  | grep '::' \\\n  | sed 's/(.*//g' \\\n  | awk -F'::' '{print $1\"::\"$2}' \\\n  | sort -u\nMost are in QPanda3, but we do see a use of Eigen for linear algebra and fmt for string formatting. You can also see a bunch of functions like QPanda3::translate_rxx_to_cx_rz_h, which correspond to the built-in gate rewrite rules.\nExpand below to see all the symbols.\n\n\nList of symbols\n\n_compute_theta:: _dec_uc_sqg:: calculate_score:: check_gate_connected:: CompilerOriginIR::IRQProgNodeCollectorParser CompilerOriginIR::OriginIRScanner CompilerParser::Parser CompilerParser::Scanner dfs:: Eigen::internal fmt::v11 generateBinarySeries:: generateCombinations:: longest_path_dfs:: modArray:: pybind11::error_already_set QPanda3::amplitude_damping_error QPanda3::BasicTranslationPass QPanda3::block_index_map_key QPanda3::cancellation_map_key QPanda3::ChipBackend QPanda3::CPUAvx2 QPanda3::CPUQVM QPanda3::create_gate QPanda3::DAG QPanda3::DAGNode QPanda3::DAGQCircuit QPanda3::decoherence_error QPanda3::decompose QPanda3::decompose_to_RPhi QPanda3::decomposeTensorProduct QPanda3::DefaultTranspilationPasses QPanda3::depolarizing_error QPanda3::diagonal_decomposition QPanda3::DrawByLayer QPanda3::DrawPicture QPanda3::EquivalenceLibrary QPanda3::GateUnitaryMatrix QPanda3::GateUnitaryMatrixUtils QPanda3::generate_topology QPanda3::get_precessor_single_gates_map QPanda3::get_unitary_matrix QPanda3::get_unitary_matrix_order QPanda3::GetUsedQubits QPanda3::InitPass QPanda3::is_double_gate QPanda3::is_single_gate QPanda3::is_supported_avx2 QPanda3::is_tensor_product QPanda3::isometry_decomposition QPanda3::IsometryDecomposition QPanda3::Karus QPanda3::LayoutPass QPanda3::LinearDepthDecomposition QPanda3::MPS_Tensor QPanda3::NameGenerator QPanda3::NoiseCircuit QPanda3::NoiseModel QPanda3::NoiseUtils QPanda3::OCircuitFusion QPanda3::operator&lt;&lt; QPanda3::OptimizationPass QPanda3::pauli_x_error QPanda3::pauli_y_error QPanda3::pauli_z_error QPanda3::phase_damping_error QPanda3::pre_handle QPanda3::QCircuit QPanda3::QGate QPanda3::QGateImplementation QPanda3::QMeasure QPanda3::QOracle QPanda3::QPandaOptions QPanda3::QProg QPanda3::QProgProcessor QPanda3::QResult QPanda3::QSDecomposition QPanda3::QStateCPU QPanda3::QStateMPS QPanda3::QuantumError QPanda3::QuantumInformation QPanda3::QuantumRegister QPanda3::QuantumState QPanda3::remap_qgate QPanda3::RoutingPass QPanda3::SabrePreLayout QPanda3::SchedulingPass QPanda3::SingletonPermutation QPanda3::task QPanda3::translate_ch_to_cx_h_p QPanda3::translate_cp_to_cx_p QPanda3::translate_crphi_to_oracle QPanda3::translate_crx_to_cx_u1_u3 QPanda3::translate_cry_to_cx_ry QPanda3::translate_crz_to_cx_rz QPanda3::translate_cu1_to_cx_u1 QPanda3::translate_cu2_to_cx_u1_u3 QPanda3::translate_cu3_to_cx_u1_u3 QPanda3::translate_cu4_to_cx_p_u3 QPanda3::translate_cx_to_cp_u3 QPanda3::translate_cx_to_cz_h QPanda3::translate_cx_to_cz_rz_x1 QPanda3::translate_cy_to_cx_p QPanda3::translate_cz_to_cx_h QPanda3::translate_h_to_rx_ry QPanda3::translate_h_to_RZ_X1 QPanda3::translate_h_to_u2 QPanda3::translate_i_to_u1 QPanda3::translate_iswap_to_cx_s_h QPanda3::translate_oracle_to_cz_u4 QPanda3::translate_p_to_RZ QPanda3::translate_p_to_u1 QPanda3::translate_rphi_to_u3 QPanda3::translate_rx_to_rz_x1 QPanda3::translate_rx_to_u3 QPanda3::translate_rxx_to_cx_rz_h QPanda3::translate_ry_to_rz_x1 QPanda3::translate_ry_to_u3 QPanda3::translate_ryy_to_cx_rx_rz QPanda3::translate_rz_to_u1 QPanda3::translate_rzx_to_cx_rz_h QPanda3::translate_rzz_to_cx_rz QPanda3::translate_s_to_RZ QPanda3::translate_s_to_u1 QPanda3::translate_sqiswap_to_cz_rz_x1 QPanda3::translate_swap_to_cx QPanda3::translate_t_to_RZ QPanda3::translate_t_to_u1 QPanda3::translate_u1_to_u3 QPanda3::translate_u2_to_u3 QPanda3::translate_u3_to_rphi_rz QPanda3::translate_u3_to_rx_rz QPanda3::translate_u3_to_u4 QPanda3::translate_u3_to_x1_rz QPanda3::translate_u4_to_u3 QPanda3::translate_u4_to_x1_rz QPanda3::translate_x_to_rx QPanda3::translate_x_to_u3 QPanda3::translate_x_to_x1 QPanda3::translate_x1_to_rphi QPanda3::translate_x1_to_rx QPanda3::translate_y_to_ry QPanda3::translate_y_to_u3 QPanda3::translate_y_to_x1_rz QPanda3::translate_y1_to_rz_x1 QPanda3::translate_z_to_rz QPanda3::translate_z_to_u1 QPanda3::translate_z1_to_rz QPanda3::TranspilationPass QPanda3::Transpiler QPanda3::TwoQubitBasisDecomposer QPanda3::TwoQubitWeylDecomposition QPanda3::uc_decomposition QPanda3::ucry_circuit QPanda3::ucry_decomposition QPanda3::ucrz_decomposition QPanda3::unroll_quantum_circuit QPandaCompiler::QProgNodeCollector std::__1 Utils::convert_int_to_str Utils::convert_uinteger_to_binary Utils::number_to_base_string WriteQCircuitTextFile::get_instance"
  },
  {
    "objectID": "posts/profiling-quantum-compilers-pyqpanda/index.html#tldr",
    "href": "posts/profiling-quantum-compilers-pyqpanda/index.html#tldr",
    "title": "Profiling Quantum Compilers: pyqpanda",
    "section": "TL;DR",
    "text": "TL;DR\nThe most surprising finding for pyqpanda3 is that the vast majority of the runtime (71%) was spent in the convert_qasm_string_to_qprog function parsing the QASM input, not optimizing the circuit. This is due to overhead in its C++ implementation, specifically from using standard library streams and locales, which could be made leaner for a fixed format like QASM.\nThe actual transpilation is remarkably fast. While it‚Äôs closed source, profiling and demangling the library revealed a multi-stage strategy involving a DAG representation, unitary synthesis, and Weyl decomposition. This peek under the hood also showed its reliance on the popular Eigen and fmt libraries. But it would be great to better understand how these steps are combined to give such strong performance!\nAI Disclaimer: I‚Äôve used LLMs to review and give feedback on this post, but the original content was written by me."
  }
]